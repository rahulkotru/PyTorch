{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\2_Miniconda\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\2_Miniconda\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MNIST(root='data/',download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset=MNIST(root='data/',train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label=dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MNIST(root='data/',train=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label =dataset[0]\n",
    "print(img_tensor.shape,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:,10:15,10:15])\n",
    "print(torch.max(img_tensor),torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 997, 2085, 2755, ..., 3856, 5679, 3645])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=6000\n",
    "val_pct=0.2\n",
    "n_val=int(val_pct*n)\n",
    "idxs=np.random.permutation(n)\n",
    "idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    n_val=int(val_pct*n)\n",
    "    idxs=np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices=split_indices(len(dataset),val_pct=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n",
      "Sample val indices:  [56916 18251 38650 23889 35545 42372 53673 30384 48666  7399 25653 34816\n",
      "  3521 34775 53316 48806 43298 42647 23288  8974]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices),len(val_indices))\n",
    "print('Sample val indices: ',val_indices[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_sampler=SubsetRandomSampler(train_indices)\n",
    "train_loader=DataLoader(dataset,batch_size,sampler=train_sampler)\n",
    "\n",
    "val_sampler=SubsetRandomSampler(val_indices)\n",
    "val_loader=DataLoader(dataset,batch_size,sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size=28*28\n",
    "num_classes=10\n",
    "\n",
    "model=nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0072, -0.0197,  0.0316,  ..., -0.0050,  0.0357, -0.0127],\n",
       "        [-0.0015,  0.0138,  0.0333,  ...,  0.0082, -0.0019, -0.0190],\n",
       "        [-0.0290, -0.0018, -0.0059,  ...,  0.0175, -0.0003,  0.0057],\n",
       "        ...,\n",
       "        [ 0.0080,  0.0264,  0.0258,  ...,  0.0020,  0.0152, -0.0114],\n",
       "        [ 0.0226, -0.0357, -0.0080,  ..., -0.0174, -0.0208,  0.0032],\n",
       "        [ 0.0082, -0.0091,  0.0222,  ..., -0.0012, -0.0266, -0.0350]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0235,  0.0084,  0.0268,  0.0243,  0.0049, -0.0234,  0.0169, -0.0275,\n",
       "        -0.0290, -0.0205], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 6, 7, 6, 1, 3, 6, 4, 5, 4, 6, 4, 4, 0, 9, 9, 9, 9, 3, 6, 6, 6, 5,\n",
      "        1, 5, 4, 7, 5, 1, 2, 9, 0, 6, 2, 0, 4, 2, 2, 0, 2, 9, 6, 8, 9, 9, 2, 9,\n",
      "        9, 6, 0, 9, 0, 5, 4, 8, 4, 2, 4, 4, 1, 7, 5, 7, 9, 5, 9, 3, 6, 2, 7, 6,\n",
      "        4, 7, 2, 2, 3, 6, 7, 6, 8, 2, 6, 3, 9, 8, 9, 4, 2, 7, 7, 0, 6, 1, 9, 5,\n",
      "        3, 0, 1, 0])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2800x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1464/3188453590.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\2_Miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\2_Miniconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\2_Miniconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2800x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs=model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "class MnistModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(input_size)\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(input_size,num_classes)\n",
    "\n",
    "    def forward(self,xb):\n",
    "        \n",
    "        xb=xb.reshape(-1,784)\n",
    "        out=self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model=MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0006,  0.0029, -0.0067,  ..., -0.0182,  0.0094,  0.0066],\n",
       "         [-0.0261,  0.0134, -0.0162,  ..., -0.0205, -0.0272,  0.0110],\n",
       "         [ 0.0012,  0.0085,  0.0249,  ..., -0.0066, -0.0171,  0.0024],\n",
       "         ...,\n",
       "         [ 0.0295, -0.0108, -0.0100,  ..., -0.0277, -0.0114,  0.0076],\n",
       "         [-0.0272,  0.0036,  0.0021,  ...,  0.0060, -0.0202,  0.0046],\n",
       "         [ 0.0031,  0.0088,  0.0300,  ..., -0.0334, -0.0277,  0.0270]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0196,  0.0187, -0.0347, -0.0213,  0.0314, -0.0067,  0.0042, -0.0156,\n",
       "          0.0256,  0.0227], requires_grad=True)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape:  torch.Size([100, 10])\n",
      "Sample outputs:\n",
      " tensor([[-0.2559,  0.2447, -0.0024,  0.2525,  0.3066, -0.0949,  0.1173,  0.0169,\n",
      "          0.0188,  0.0704],\n",
      "        [-0.3550,  0.1096,  0.0980,  0.2074,  0.0858, -0.2493,  0.2912,  0.0657,\n",
      "          0.1202,  0.1682]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs=model(images)\n",
    "    \n",
    "    break\n",
    "print('outputs.shape: ',outputs.shape)\n",
    "print(\"Sample outputs:\\n\",outputs[:2].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.0714, 0.1179, 0.0921, 0.1188, 0.1254, 0.0839, 0.1038, 0.0938, 0.0940,\n",
      "         0.0990],\n",
      "        [0.0653, 0.1039, 0.1027, 0.1146, 0.1015, 0.0726, 0.1246, 0.0995, 0.1050,\n",
      "         0.1102]])\n",
      "Sum:  1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "probs=F.softmax(outputs, dim=1)\n",
    "print(\"Sample probabilities:\\n\",probs[:2].data)\n",
    "print(\"Sum: \",torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6, 3, 6, 3, 9, 3, 3, 1, 3, 1, 3, 3, 1, 1, 3, 5, 3, 3, 3, 9, 7, 3, 9,\n",
      "        6, 3, 3, 1, 1, 3, 4, 8, 3, 3, 1, 3, 1, 3, 4, 3, 7, 1, 9, 9, 1, 3, 3, 3,\n",
      "        1, 9, 1, 7, 1, 1, 9, 3, 6, 3, 3, 8, 1, 3, 1, 3, 7, 8, 7, 1, 8, 1, 3, 1,\n",
      "        8, 1, 1, 6, 4, 3, 3, 6, 3, 1, 3, 6, 4, 6, 6, 5, 9, 9, 1, 7, 6, 3, 1, 3,\n",
      "        9, 1, 3, 3])\n",
      "tensor([0.1254, 0.1246, 0.1212, 0.1228, 0.1318, 0.1318, 0.1473, 0.1416, 0.1222,\n",
      "        0.1189, 0.1264, 0.1245, 0.1551, 0.1376, 0.1152, 0.1340, 0.1257, 0.1259,\n",
      "        0.1296, 0.1219, 0.1297, 0.1245, 0.1315, 0.1250, 0.1283, 0.1560, 0.1498,\n",
      "        0.1324, 0.1319, 0.1592, 0.1343, 0.1214, 0.1428, 0.1350, 0.1233, 0.1440,\n",
      "        0.1259, 0.1441, 0.1253, 0.1296, 0.1314, 0.1325, 0.1313, 0.1303, 0.1241,\n",
      "        0.1321, 0.1415, 0.1318, 0.1375, 0.1170, 0.1330, 0.1248, 0.1276, 0.1222,\n",
      "        0.1289, 0.1417, 0.1120, 0.1212, 0.1320, 0.1287, 0.1359, 0.1293, 0.1576,\n",
      "        0.1592, 0.1202, 0.1254, 0.1263, 0.1169, 0.1297, 0.1181, 0.1357, 0.1326,\n",
      "        0.1165, 0.1312, 0.1292, 0.1462, 0.1108, 0.1573, 0.1278, 0.1315, 0.1554,\n",
      "        0.1105, 0.1139, 0.1262, 0.1331, 0.1524, 0.1244, 0.1262, 0.1286, 0.1304,\n",
      "        0.1405, 0.1237, 0.1302, 0.1537, 0.1446, 0.1556, 0.1249, 0.1435, 0.1181,\n",
      "        0.1347], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds=torch.max(probs,dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 3, 5, 4, 9, 3, 8, 4, 9, 7, 5, 6, 7, 9, 1, 9, 0, 7, 4, 3, 1, 9, 5, 0,\n",
       "        0, 7, 7, 4, 8, 9, 5, 1, 5, 6, 4, 2, 2, 5, 3, 6, 7, 5, 9, 3, 6, 4, 5, 8,\n",
       "        6, 5, 2, 9, 5, 5, 3, 8, 9, 2, 8, 1, 3, 8, 8, 5, 9, 1, 2, 6, 4, 3, 7, 8,\n",
       "        5, 8, 0, 2, 1, 7, 3, 5, 9, 4, 5, 2, 3, 8, 2, 4, 8, 9, 8, 7, 6, 7, 8, 6,\n",
       "        9, 6, 2, 8])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,  True,\n",
       "        False,  True,  True, False, False, False,  True, False, False, False])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels==preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1,l2):\n",
    "    return torch.sum(l1==l2).item()/len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds,labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3265, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss=loss_fn(outputs,labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model,loss_func,xb,yb,opt=None,metric=None):\n",
    "    preds=model(xb)\n",
    "    loss=loss_func(preds,yb)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    metric_result=None\n",
    "    if metric is not None:\n",
    "        metric_result=metric(preds,yb)\n",
    "\n",
    "    return loss.item(),len(xb), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,loss_fn,valid_dl,metric=None):\n",
    "    with torch.no_grad():\n",
    "        results=[loss_batch(model,loss_fn,xb,yb,metric=metric)\n",
    "        for xb,yb in valid_dl]\n",
    "        losses,nums,metrics=zip(*results)\n",
    "        total=np.sum(nums)\n",
    "        avg_loss=np.sum(np.multiply(losses,nums))/total\n",
    "        avg_metric=None\n",
    "        if metric is not None:\n",
    "            avg_metrics=np.sum(np.multiply(metrics,nums))/total\n",
    "    return avg_loss,total,avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _,preds=torch.max(outputs,dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3427, Accuracy: 0.0723\n"
     ]
    }
   ],
   "source": [
    "val_loss,total,val_acc=evaluate(model,loss_fn,val_loader,metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,model,loss_fn,opt,train_dl,valid_dl,metric=None):\n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_,_=loss_batch(model,loss_fn,xb,yb,opt)\n",
    "            result=evaluate(model,loss_fn,valid_dl,metric)\n",
    "            val_loss,total,val_metric=result\n",
    "            if metric is None:\n",
    "                print('Epoch[{}/{},Loss: {:.4f}]'.format(epoch+1,epochs,val_loss))\n",
    "            else:\n",
    "                print('Epoch[{}/{}, Loss: {:.4f}, {}: {:.4f}]'.format(epoch+1,epochs,val_loss,metric.__name__,val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "model=MnistModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1, Loss: 2.0549, accuracy: 0.5090]\n",
      "Epoch[1/1, Loss: 2.0540, accuracy: 0.5104]\n",
      "Epoch[1/1, Loss: 2.0530, accuracy: 0.5112]\n",
      "Epoch[1/1, Loss: 2.0521, accuracy: 0.5128]\n",
      "Epoch[1/1, Loss: 2.0513, accuracy: 0.5133]\n",
      "Epoch[1/1, Loss: 2.0504, accuracy: 0.5157]\n",
      "Epoch[1/1, Loss: 2.0496, accuracy: 0.5166]\n",
      "Epoch[1/1, Loss: 2.0488, accuracy: 0.5176]\n",
      "Epoch[1/1, Loss: 2.0479, accuracy: 0.5188]\n",
      "Epoch[1/1, Loss: 2.0470, accuracy: 0.5196]\n",
      "Epoch[1/1, Loss: 2.0460, accuracy: 0.5222]\n",
      "Epoch[1/1, Loss: 2.0451, accuracy: 0.5233]\n",
      "Epoch[1/1, Loss: 2.0443, accuracy: 0.5236]\n",
      "Epoch[1/1, Loss: 2.0434, accuracy: 0.5247]\n",
      "Epoch[1/1, Loss: 2.0424, accuracy: 0.5256]\n",
      "Epoch[1/1, Loss: 2.0415, accuracy: 0.5261]\n",
      "Epoch[1/1, Loss: 2.0405, accuracy: 0.5271]\n",
      "Epoch[1/1, Loss: 2.0395, accuracy: 0.5297]\n",
      "Epoch[1/1, Loss: 2.0388, accuracy: 0.5310]\n",
      "Epoch[1/1, Loss: 2.0379, accuracy: 0.5321]\n",
      "Epoch[1/1, Loss: 2.0370, accuracy: 0.5335]\n",
      "Epoch[1/1, Loss: 2.0362, accuracy: 0.5334]\n",
      "Epoch[1/1, Loss: 2.0354, accuracy: 0.5344]\n",
      "Epoch[1/1, Loss: 2.0345, accuracy: 0.5351]\n",
      "Epoch[1/1, Loss: 2.0337, accuracy: 0.5357]\n",
      "Epoch[1/1, Loss: 2.0328, accuracy: 0.5371]\n",
      "Epoch[1/1, Loss: 2.0319, accuracy: 0.5377]\n",
      "Epoch[1/1, Loss: 2.0311, accuracy: 0.5387]\n",
      "Epoch[1/1, Loss: 2.0302, accuracy: 0.5395]\n",
      "Epoch[1/1, Loss: 2.0294, accuracy: 0.5414]\n",
      "Epoch[1/1, Loss: 2.0285, accuracy: 0.5414]\n",
      "Epoch[1/1, Loss: 2.0277, accuracy: 0.5421]\n",
      "Epoch[1/1, Loss: 2.0269, accuracy: 0.5427]\n",
      "Epoch[1/1, Loss: 2.0260, accuracy: 0.5451]\n",
      "Epoch[1/1, Loss: 2.0251, accuracy: 0.5470]\n",
      "Epoch[1/1, Loss: 2.0243, accuracy: 0.5473]\n",
      "Epoch[1/1, Loss: 2.0235, accuracy: 0.5493]\n",
      "Epoch[1/1, Loss: 2.0226, accuracy: 0.5507]\n",
      "Epoch[1/1, Loss: 2.0218, accuracy: 0.5523]\n",
      "Epoch[1/1, Loss: 2.0210, accuracy: 0.5528]\n",
      "Epoch[1/1, Loss: 2.0201, accuracy: 0.5540]\n",
      "Epoch[1/1, Loss: 2.0192, accuracy: 0.5553]\n",
      "Epoch[1/1, Loss: 2.0183, accuracy: 0.5559]\n",
      "Epoch[1/1, Loss: 2.0174, accuracy: 0.5572]\n",
      "Epoch[1/1, Loss: 2.0166, accuracy: 0.5582]\n",
      "Epoch[1/1, Loss: 2.0157, accuracy: 0.5590]\n",
      "Epoch[1/1, Loss: 2.0148, accuracy: 0.5605]\n",
      "Epoch[1/1, Loss: 2.0140, accuracy: 0.5620]\n",
      "Epoch[1/1, Loss: 2.0132, accuracy: 0.5633]\n",
      "Epoch[1/1, Loss: 2.0123, accuracy: 0.5660]\n",
      "Epoch[1/1, Loss: 2.0115, accuracy: 0.5676]\n",
      "Epoch[1/1, Loss: 2.0106, accuracy: 0.5688]\n",
      "Epoch[1/1, Loss: 2.0098, accuracy: 0.5697]\n",
      "Epoch[1/1, Loss: 2.0090, accuracy: 0.5713]\n",
      "Epoch[1/1, Loss: 2.0082, accuracy: 0.5727]\n",
      "Epoch[1/1, Loss: 2.0073, accuracy: 0.5745]\n",
      "Epoch[1/1, Loss: 2.0064, accuracy: 0.5762]\n",
      "Epoch[1/1, Loss: 2.0056, accuracy: 0.5765]\n",
      "Epoch[1/1, Loss: 2.0048, accuracy: 0.5781]\n",
      "Epoch[1/1, Loss: 2.0039, accuracy: 0.5793]\n",
      "Epoch[1/1, Loss: 2.0031, accuracy: 0.5817]\n",
      "Epoch[1/1, Loss: 2.0023, accuracy: 0.5814]\n",
      "Epoch[1/1, Loss: 2.0015, accuracy: 0.5823]\n",
      "Epoch[1/1, Loss: 2.0006, accuracy: 0.5854]\n",
      "Epoch[1/1, Loss: 1.9998, accuracy: 0.5867]\n",
      "Epoch[1/1, Loss: 1.9989, accuracy: 0.5876]\n",
      "Epoch[1/1, Loss: 1.9980, accuracy: 0.5891]\n",
      "Epoch[1/1, Loss: 1.9971, accuracy: 0.5902]\n",
      "Epoch[1/1, Loss: 1.9963, accuracy: 0.5912]\n",
      "Epoch[1/1, Loss: 1.9955, accuracy: 0.5912]\n",
      "Epoch[1/1, Loss: 1.9947, accuracy: 0.5918]\n",
      "Epoch[1/1, Loss: 1.9939, accuracy: 0.5924]\n",
      "Epoch[1/1, Loss: 1.9931, accuracy: 0.5937]\n",
      "Epoch[1/1, Loss: 1.9923, accuracy: 0.5939]\n",
      "Epoch[1/1, Loss: 1.9915, accuracy: 0.5951]\n",
      "Epoch[1/1, Loss: 1.9906, accuracy: 0.5955]\n",
      "Epoch[1/1, Loss: 1.9897, accuracy: 0.5967]\n",
      "Epoch[1/1, Loss: 1.9889, accuracy: 0.5978]\n",
      "Epoch[1/1, Loss: 1.9880, accuracy: 0.5989]\n",
      "Epoch[1/1, Loss: 1.9871, accuracy: 0.5999]\n",
      "Epoch[1/1, Loss: 1.9863, accuracy: 0.5999]\n",
      "Epoch[1/1, Loss: 1.9854, accuracy: 0.6010]\n",
      "Epoch[1/1, Loss: 1.9846, accuracy: 0.6017]\n",
      "Epoch[1/1, Loss: 1.9838, accuracy: 0.6024]\n",
      "Epoch[1/1, Loss: 1.9830, accuracy: 0.6032]\n",
      "Epoch[1/1, Loss: 1.9822, accuracy: 0.6045]\n",
      "Epoch[1/1, Loss: 1.9813, accuracy: 0.6052]\n",
      "Epoch[1/1, Loss: 1.9805, accuracy: 0.6057]\n",
      "Epoch[1/1, Loss: 1.9798, accuracy: 0.6066]\n",
      "Epoch[1/1, Loss: 1.9789, accuracy: 0.6057]\n",
      "Epoch[1/1, Loss: 1.9780, accuracy: 0.6069]\n",
      "Epoch[1/1, Loss: 1.9772, accuracy: 0.6078]\n",
      "Epoch[1/1, Loss: 1.9764, accuracy: 0.6088]\n",
      "Epoch[1/1, Loss: 1.9755, accuracy: 0.6100]\n",
      "Epoch[1/1, Loss: 1.9747, accuracy: 0.6102]\n",
      "Epoch[1/1, Loss: 1.9739, accuracy: 0.6120]\n",
      "Epoch[1/1, Loss: 1.9732, accuracy: 0.6130]\n",
      "Epoch[1/1, Loss: 1.9725, accuracy: 0.6129]\n",
      "Epoch[1/1, Loss: 1.9717, accuracy: 0.6147]\n",
      "Epoch[1/1, Loss: 1.9709, accuracy: 0.6158]\n",
      "Epoch[1/1, Loss: 1.9701, accuracy: 0.6162]\n",
      "Epoch[1/1, Loss: 1.9692, accuracy: 0.6162]\n",
      "Epoch[1/1, Loss: 1.9684, accuracy: 0.6168]\n",
      "Epoch[1/1, Loss: 1.9677, accuracy: 0.6182]\n",
      "Epoch[1/1, Loss: 1.9669, accuracy: 0.6189]\n",
      "Epoch[1/1, Loss: 1.9660, accuracy: 0.6198]\n",
      "Epoch[1/1, Loss: 1.9653, accuracy: 0.6188]\n",
      "Epoch[1/1, Loss: 1.9645, accuracy: 0.6194]\n",
      "Epoch[1/1, Loss: 1.9637, accuracy: 0.6205]\n",
      "Epoch[1/1, Loss: 1.9629, accuracy: 0.6209]\n",
      "Epoch[1/1, Loss: 1.9621, accuracy: 0.6213]\n",
      "Epoch[1/1, Loss: 1.9613, accuracy: 0.6229]\n",
      "Epoch[1/1, Loss: 1.9604, accuracy: 0.6244]\n",
      "Epoch[1/1, Loss: 1.9597, accuracy: 0.6249]\n",
      "Epoch[1/1, Loss: 1.9588, accuracy: 0.6257]\n",
      "Epoch[1/1, Loss: 1.9580, accuracy: 0.6270]\n",
      "Epoch[1/1, Loss: 1.9572, accuracy: 0.6266]\n",
      "Epoch[1/1, Loss: 1.9564, accuracy: 0.6272]\n",
      "Epoch[1/1, Loss: 1.9556, accuracy: 0.6273]\n",
      "Epoch[1/1, Loss: 1.9548, accuracy: 0.6278]\n",
      "Epoch[1/1, Loss: 1.9541, accuracy: 0.6292]\n",
      "Epoch[1/1, Loss: 1.9532, accuracy: 0.6308]\n",
      "Epoch[1/1, Loss: 1.9525, accuracy: 0.6302]\n",
      "Epoch[1/1, Loss: 1.9517, accuracy: 0.6306]\n",
      "Epoch[1/1, Loss: 1.9509, accuracy: 0.6322]\n",
      "Epoch[1/1, Loss: 1.9501, accuracy: 0.6334]\n",
      "Epoch[1/1, Loss: 1.9494, accuracy: 0.6332]\n",
      "Epoch[1/1, Loss: 1.9485, accuracy: 0.6351]\n",
      "Epoch[1/1, Loss: 1.9477, accuracy: 0.6350]\n",
      "Epoch[1/1, Loss: 1.9469, accuracy: 0.6353]\n",
      "Epoch[1/1, Loss: 1.9461, accuracy: 0.6352]\n",
      "Epoch[1/1, Loss: 1.9452, accuracy: 0.6358]\n",
      "Epoch[1/1, Loss: 1.9444, accuracy: 0.6372]\n",
      "Epoch[1/1, Loss: 1.9436, accuracy: 0.6392]\n",
      "Epoch[1/1, Loss: 1.9428, accuracy: 0.6393]\n",
      "Epoch[1/1, Loss: 1.9419, accuracy: 0.6403]\n",
      "Epoch[1/1, Loss: 1.9412, accuracy: 0.6408]\n",
      "Epoch[1/1, Loss: 1.9404, accuracy: 0.6412]\n",
      "Epoch[1/1, Loss: 1.9396, accuracy: 0.6421]\n",
      "Epoch[1/1, Loss: 1.9388, accuracy: 0.6434]\n",
      "Epoch[1/1, Loss: 1.9381, accuracy: 0.6442]\n",
      "Epoch[1/1, Loss: 1.9372, accuracy: 0.6462]\n",
      "Epoch[1/1, Loss: 1.9365, accuracy: 0.6472]\n",
      "Epoch[1/1, Loss: 1.9357, accuracy: 0.6490]\n",
      "Epoch[1/1, Loss: 1.9350, accuracy: 0.6486]\n",
      "Epoch[1/1, Loss: 1.9342, accuracy: 0.6495]\n",
      "Epoch[1/1, Loss: 1.9335, accuracy: 0.6503]\n",
      "Epoch[1/1, Loss: 1.9326, accuracy: 0.6515]\n",
      "Epoch[1/1, Loss: 1.9318, accuracy: 0.6524]\n",
      "Epoch[1/1, Loss: 1.9310, accuracy: 0.6528]\n",
      "Epoch[1/1, Loss: 1.9301, accuracy: 0.6544]\n",
      "Epoch[1/1, Loss: 1.9293, accuracy: 0.6548]\n",
      "Epoch[1/1, Loss: 1.9285, accuracy: 0.6556]\n",
      "Epoch[1/1, Loss: 1.9277, accuracy: 0.6559]\n",
      "Epoch[1/1, Loss: 1.9270, accuracy: 0.6561]\n",
      "Epoch[1/1, Loss: 1.9262, accuracy: 0.6566]\n",
      "Epoch[1/1, Loss: 1.9255, accuracy: 0.6574]\n",
      "Epoch[1/1, Loss: 1.9246, accuracy: 0.6579]\n",
      "Epoch[1/1, Loss: 1.9238, accuracy: 0.6589]\n",
      "Epoch[1/1, Loss: 1.9230, accuracy: 0.6595]\n",
      "Epoch[1/1, Loss: 1.9223, accuracy: 0.6606]\n",
      "Epoch[1/1, Loss: 1.9216, accuracy: 0.6611]\n",
      "Epoch[1/1, Loss: 1.9208, accuracy: 0.6612]\n",
      "Epoch[1/1, Loss: 1.9199, accuracy: 0.6621]\n",
      "Epoch[1/1, Loss: 1.9191, accuracy: 0.6631]\n",
      "Epoch[1/1, Loss: 1.9183, accuracy: 0.6640]\n",
      "Epoch[1/1, Loss: 1.9176, accuracy: 0.6642]\n",
      "Epoch[1/1, Loss: 1.9168, accuracy: 0.6643]\n",
      "Epoch[1/1, Loss: 1.9160, accuracy: 0.6653]\n",
      "Epoch[1/1, Loss: 1.9153, accuracy: 0.6667]\n",
      "Epoch[1/1, Loss: 1.9145, accuracy: 0.6668]\n",
      "Epoch[1/1, Loss: 1.9138, accuracy: 0.6672]\n",
      "Epoch[1/1, Loss: 1.9131, accuracy: 0.6677]\n",
      "Epoch[1/1, Loss: 1.9123, accuracy: 0.6683]\n",
      "Epoch[1/1, Loss: 1.9116, accuracy: 0.6690]\n",
      "Epoch[1/1, Loss: 1.9108, accuracy: 0.6695]\n",
      "Epoch[1/1, Loss: 1.9101, accuracy: 0.6703]\n",
      "Epoch[1/1, Loss: 1.9093, accuracy: 0.6706]\n",
      "Epoch[1/1, Loss: 1.9085, accuracy: 0.6717]\n",
      "Epoch[1/1, Loss: 1.9077, accuracy: 0.6716]\n",
      "Epoch[1/1, Loss: 1.9069, accuracy: 0.6721]\n",
      "Epoch[1/1, Loss: 1.9062, accuracy: 0.6741]\n",
      "Epoch[1/1, Loss: 1.9054, accuracy: 0.6742]\n",
      "Epoch[1/1, Loss: 1.9047, accuracy: 0.6741]\n",
      "Epoch[1/1, Loss: 1.9039, accuracy: 0.6752]\n",
      "Epoch[1/1, Loss: 1.9031, accuracy: 0.6755]\n",
      "Epoch[1/1, Loss: 1.9023, accuracy: 0.6763]\n",
      "Epoch[1/1, Loss: 1.9015, accuracy: 0.6769]\n",
      "Epoch[1/1, Loss: 1.9008, accuracy: 0.6771]\n",
      "Epoch[1/1, Loss: 1.9001, accuracy: 0.6776]\n",
      "Epoch[1/1, Loss: 1.8993, accuracy: 0.6781]\n",
      "Epoch[1/1, Loss: 1.8986, accuracy: 0.6770]\n",
      "Epoch[1/1, Loss: 1.8978, accuracy: 0.6781]\n",
      "Epoch[1/1, Loss: 1.8971, accuracy: 0.6781]\n",
      "Epoch[1/1, Loss: 1.8963, accuracy: 0.6788]\n",
      "Epoch[1/1, Loss: 1.8955, accuracy: 0.6795]\n",
      "Epoch[1/1, Loss: 1.8947, accuracy: 0.6800]\n",
      "Epoch[1/1, Loss: 1.8940, accuracy: 0.6806]\n",
      "Epoch[1/1, Loss: 1.8933, accuracy: 0.6814]\n",
      "Epoch[1/1, Loss: 1.8925, accuracy: 0.6814]\n",
      "Epoch[1/1, Loss: 1.8917, accuracy: 0.6814]\n",
      "Epoch[1/1, Loss: 1.8910, accuracy: 0.6813]\n",
      "Epoch[1/1, Loss: 1.8902, accuracy: 0.6816]\n",
      "Epoch[1/1, Loss: 1.8896, accuracy: 0.6823]\n",
      "Epoch[1/1, Loss: 1.8888, accuracy: 0.6832]\n",
      "Epoch[1/1, Loss: 1.8881, accuracy: 0.6832]\n",
      "Epoch[1/1, Loss: 1.8874, accuracy: 0.6839]\n",
      "Epoch[1/1, Loss: 1.8866, accuracy: 0.6842]\n",
      "Epoch[1/1, Loss: 1.8858, accuracy: 0.6843]\n",
      "Epoch[1/1, Loss: 1.8850, accuracy: 0.6843]\n",
      "Epoch[1/1, Loss: 1.8843, accuracy: 0.6847]\n",
      "Epoch[1/1, Loss: 1.8835, accuracy: 0.6860]\n",
      "Epoch[1/1, Loss: 1.8827, accuracy: 0.6857]\n",
      "Epoch[1/1, Loss: 1.8820, accuracy: 0.6867]\n",
      "Epoch[1/1, Loss: 1.8813, accuracy: 0.6870]\n",
      "Epoch[1/1, Loss: 1.8806, accuracy: 0.6875]\n",
      "Epoch[1/1, Loss: 1.8799, accuracy: 0.6878]\n",
      "Epoch[1/1, Loss: 1.8792, accuracy: 0.6883]\n",
      "Epoch[1/1, Loss: 1.8784, accuracy: 0.6885]\n",
      "Epoch[1/1, Loss: 1.8777, accuracy: 0.6892]\n",
      "Epoch[1/1, Loss: 1.8769, accuracy: 0.6897]\n",
      "Epoch[1/1, Loss: 1.8761, accuracy: 0.6913]\n",
      "Epoch[1/1, Loss: 1.8753, accuracy: 0.6909]\n",
      "Epoch[1/1, Loss: 1.8746, accuracy: 0.6909]\n",
      "Epoch[1/1, Loss: 1.8739, accuracy: 0.6912]\n",
      "Epoch[1/1, Loss: 1.8732, accuracy: 0.6919]\n",
      "Epoch[1/1, Loss: 1.8725, accuracy: 0.6919]\n",
      "Epoch[1/1, Loss: 1.8718, accuracy: 0.6923]\n",
      "Epoch[1/1, Loss: 1.8710, accuracy: 0.6922]\n",
      "Epoch[1/1, Loss: 1.8703, accuracy: 0.6925]\n",
      "Epoch[1/1, Loss: 1.8696, accuracy: 0.6936]\n",
      "Epoch[1/1, Loss: 1.8688, accuracy: 0.6944]\n",
      "Epoch[1/1, Loss: 1.8681, accuracy: 0.6939]\n",
      "Epoch[1/1, Loss: 1.8674, accuracy: 0.6951]\n",
      "Epoch[1/1, Loss: 1.8666, accuracy: 0.6956]\n",
      "Epoch[1/1, Loss: 1.8659, accuracy: 0.6956]\n",
      "Epoch[1/1, Loss: 1.8652, accuracy: 0.6967]\n",
      "Epoch[1/1, Loss: 1.8645, accuracy: 0.6964]\n",
      "Epoch[1/1, Loss: 1.8638, accuracy: 0.6969]\n",
      "Epoch[1/1, Loss: 1.8631, accuracy: 0.6975]\n",
      "Epoch[1/1, Loss: 1.8624, accuracy: 0.6979]\n",
      "Epoch[1/1, Loss: 1.8617, accuracy: 0.6980]\n",
      "Epoch[1/1, Loss: 1.8609, accuracy: 0.6980]\n",
      "Epoch[1/1, Loss: 1.8602, accuracy: 0.6989]\n",
      "Epoch[1/1, Loss: 1.8595, accuracy: 0.6993]\n",
      "Epoch[1/1, Loss: 1.8588, accuracy: 0.7007]\n",
      "Epoch[1/1, Loss: 1.8580, accuracy: 0.7005]\n",
      "Epoch[1/1, Loss: 1.8572, accuracy: 0.7011]\n",
      "Epoch[1/1, Loss: 1.8565, accuracy: 0.7018]\n",
      "Epoch[1/1, Loss: 1.8558, accuracy: 0.7023]\n",
      "Epoch[1/1, Loss: 1.8550, accuracy: 0.7029]\n",
      "Epoch[1/1, Loss: 1.8543, accuracy: 0.7032]\n",
      "Epoch[1/1, Loss: 1.8536, accuracy: 0.7037]\n",
      "Epoch[1/1, Loss: 1.8528, accuracy: 0.7043]\n",
      "Epoch[1/1, Loss: 1.8521, accuracy: 0.7047]\n",
      "Epoch[1/1, Loss: 1.8514, accuracy: 0.7057]\n",
      "Epoch[1/1, Loss: 1.8506, accuracy: 0.7062]\n",
      "Epoch[1/1, Loss: 1.8500, accuracy: 0.7062]\n",
      "Epoch[1/1, Loss: 1.8493, accuracy: 0.7072]\n",
      "Epoch[1/1, Loss: 1.8486, accuracy: 0.7079]\n",
      "Epoch[1/1, Loss: 1.8479, accuracy: 0.7078]\n",
      "Epoch[1/1, Loss: 1.8471, accuracy: 0.7079]\n",
      "Epoch[1/1, Loss: 1.8464, accuracy: 0.7081]\n",
      "Epoch[1/1, Loss: 1.8456, accuracy: 0.7089]\n",
      "Epoch[1/1, Loss: 1.8450, accuracy: 0.7087]\n",
      "Epoch[1/1, Loss: 1.8443, accuracy: 0.7096]\n",
      "Epoch[1/1, Loss: 1.8436, accuracy: 0.7101]\n",
      "Epoch[1/1, Loss: 1.8428, accuracy: 0.7102]\n",
      "Epoch[1/1, Loss: 1.8421, accuracy: 0.7103]\n",
      "Epoch[1/1, Loss: 1.8413, accuracy: 0.7108]\n",
      "Epoch[1/1, Loss: 1.8406, accuracy: 0.7119]\n",
      "Epoch[1/1, Loss: 1.8399, accuracy: 0.7113]\n",
      "Epoch[1/1, Loss: 1.8391, accuracy: 0.7108]\n",
      "Epoch[1/1, Loss: 1.8384, accuracy: 0.7107]\n",
      "Epoch[1/1, Loss: 1.8376, accuracy: 0.7113]\n",
      "Epoch[1/1, Loss: 1.8370, accuracy: 0.7117]\n",
      "Epoch[1/1, Loss: 1.8362, accuracy: 0.7118]\n",
      "Epoch[1/1, Loss: 1.8355, accuracy: 0.7124]\n",
      "Epoch[1/1, Loss: 1.8348, accuracy: 0.7121]\n",
      "Epoch[1/1, Loss: 1.8340, accuracy: 0.7120]\n",
      "Epoch[1/1, Loss: 1.8333, accuracy: 0.7132]\n",
      "Epoch[1/1, Loss: 1.8326, accuracy: 0.7128]\n",
      "Epoch[1/1, Loss: 1.8319, accuracy: 0.7129]\n",
      "Epoch[1/1, Loss: 1.8312, accuracy: 0.7131]\n",
      "Epoch[1/1, Loss: 1.8305, accuracy: 0.7137]\n",
      "Epoch[1/1, Loss: 1.8298, accuracy: 0.7140]\n",
      "Epoch[1/1, Loss: 1.8291, accuracy: 0.7144]\n",
      "Epoch[1/1, Loss: 1.8284, accuracy: 0.7146]\n",
      "Epoch[1/1, Loss: 1.8276, accuracy: 0.7146]\n",
      "Epoch[1/1, Loss: 1.8270, accuracy: 0.7147]\n",
      "Epoch[1/1, Loss: 1.8262, accuracy: 0.7146]\n",
      "Epoch[1/1, Loss: 1.8255, accuracy: 0.7150]\n",
      "Epoch[1/1, Loss: 1.8249, accuracy: 0.7148]\n",
      "Epoch[1/1, Loss: 1.8242, accuracy: 0.7156]\n",
      "Epoch[1/1, Loss: 1.8235, accuracy: 0.7157]\n",
      "Epoch[1/1, Loss: 1.8228, accuracy: 0.7156]\n",
      "Epoch[1/1, Loss: 1.8221, accuracy: 0.7157]\n",
      "Epoch[1/1, Loss: 1.8214, accuracy: 0.7160]\n",
      "Epoch[1/1, Loss: 1.8208, accuracy: 0.7167]\n",
      "Epoch[1/1, Loss: 1.8201, accuracy: 0.7173]\n",
      "Epoch[1/1, Loss: 1.8193, accuracy: 0.7178]\n",
      "Epoch[1/1, Loss: 1.8187, accuracy: 0.7183]\n",
      "Epoch[1/1, Loss: 1.8180, accuracy: 0.7183]\n",
      "Epoch[1/1, Loss: 1.8173, accuracy: 0.7183]\n",
      "Epoch[1/1, Loss: 1.8166, accuracy: 0.7188]\n",
      "Epoch[1/1, Loss: 1.8159, accuracy: 0.7192]\n",
      "Epoch[1/1, Loss: 1.8152, accuracy: 0.7190]\n",
      "Epoch[1/1, Loss: 1.8145, accuracy: 0.7190]\n",
      "Epoch[1/1, Loss: 1.8138, accuracy: 0.7196]\n",
      "Epoch[1/1, Loss: 1.8131, accuracy: 0.7197]\n",
      "Epoch[1/1, Loss: 1.8124, accuracy: 0.7200]\n",
      "Epoch[1/1, Loss: 1.8117, accuracy: 0.7203]\n",
      "Epoch[1/1, Loss: 1.8110, accuracy: 0.7205]\n",
      "Epoch[1/1, Loss: 1.8103, accuracy: 0.7203]\n",
      "Epoch[1/1, Loss: 1.8096, accuracy: 0.7208]\n",
      "Epoch[1/1, Loss: 1.8090, accuracy: 0.7208]\n",
      "Epoch[1/1, Loss: 1.8083, accuracy: 0.7214]\n",
      "Epoch[1/1, Loss: 1.8076, accuracy: 0.7210]\n",
      "Epoch[1/1, Loss: 1.8069, accuracy: 0.7206]\n",
      "Epoch[1/1, Loss: 1.8062, accuracy: 0.7204]\n",
      "Epoch[1/1, Loss: 1.8055, accuracy: 0.7205]\n",
      "Epoch[1/1, Loss: 1.8048, accuracy: 0.7207]\n",
      "Epoch[1/1, Loss: 1.8041, accuracy: 0.7212]\n",
      "Epoch[1/1, Loss: 1.8035, accuracy: 0.7216]\n",
      "Epoch[1/1, Loss: 1.8028, accuracy: 0.7213]\n",
      "Epoch[1/1, Loss: 1.8021, accuracy: 0.7214]\n",
      "Epoch[1/1, Loss: 1.8014, accuracy: 0.7218]\n",
      "Epoch[1/1, Loss: 1.8008, accuracy: 0.7228]\n",
      "Epoch[1/1, Loss: 1.8001, accuracy: 0.7238]\n",
      "Epoch[1/1, Loss: 1.7994, accuracy: 0.7234]\n",
      "Epoch[1/1, Loss: 1.7987, accuracy: 0.7240]\n",
      "Epoch[1/1, Loss: 1.7980, accuracy: 0.7240]\n",
      "Epoch[1/1, Loss: 1.7973, accuracy: 0.7244]\n",
      "Epoch[1/1, Loss: 1.7966, accuracy: 0.7246]\n",
      "Epoch[1/1, Loss: 1.7959, accuracy: 0.7245]\n",
      "Epoch[1/1, Loss: 1.7953, accuracy: 0.7244]\n",
      "Epoch[1/1, Loss: 1.7945, accuracy: 0.7246]\n",
      "Epoch[1/1, Loss: 1.7939, accuracy: 0.7248]\n",
      "Epoch[1/1, Loss: 1.7932, accuracy: 0.7252]\n",
      "Epoch[1/1, Loss: 1.7925, accuracy: 0.7255]\n",
      "Epoch[1/1, Loss: 1.7919, accuracy: 0.7252]\n",
      "Epoch[1/1, Loss: 1.7912, accuracy: 0.7258]\n",
      "Epoch[1/1, Loss: 1.7905, accuracy: 0.7265]\n",
      "Epoch[1/1, Loss: 1.7898, accuracy: 0.7267]\n",
      "Epoch[1/1, Loss: 1.7892, accuracy: 0.7269]\n",
      "Epoch[1/1, Loss: 1.7885, accuracy: 0.7269]\n",
      "Epoch[1/1, Loss: 1.7878, accuracy: 0.7277]\n",
      "Epoch[1/1, Loss: 1.7871, accuracy: 0.7272]\n",
      "Epoch[1/1, Loss: 1.7864, accuracy: 0.7278]\n",
      "Epoch[1/1, Loss: 1.7857, accuracy: 0.7278]\n",
      "Epoch[1/1, Loss: 1.7851, accuracy: 0.7284]\n",
      "Epoch[1/1, Loss: 1.7844, accuracy: 0.7288]\n",
      "Epoch[1/1, Loss: 1.7838, accuracy: 0.7297]\n",
      "Epoch[1/1, Loss: 1.7831, accuracy: 0.7295]\n",
      "Epoch[1/1, Loss: 1.7824, accuracy: 0.7295]\n",
      "Epoch[1/1, Loss: 1.7818, accuracy: 0.7298]\n",
      "Epoch[1/1, Loss: 1.7811, accuracy: 0.7302]\n",
      "Epoch[1/1, Loss: 1.7804, accuracy: 0.7308]\n",
      "Epoch[1/1, Loss: 1.7798, accuracy: 0.7304]\n",
      "Epoch[1/1, Loss: 1.7791, accuracy: 0.7308]\n",
      "Epoch[1/1, Loss: 1.7784, accuracy: 0.7311]\n",
      "Epoch[1/1, Loss: 1.7778, accuracy: 0.7312]\n",
      "Epoch[1/1, Loss: 1.7771, accuracy: 0.7311]\n",
      "Epoch[1/1, Loss: 1.7765, accuracy: 0.7315]\n",
      "Epoch[1/1, Loss: 1.7758, accuracy: 0.7319]\n",
      "Epoch[1/1, Loss: 1.7752, accuracy: 0.7318]\n",
      "Epoch[1/1, Loss: 1.7745, accuracy: 0.7322]\n",
      "Epoch[1/1, Loss: 1.7738, accuracy: 0.7323]\n",
      "Epoch[1/1, Loss: 1.7731, accuracy: 0.7320]\n",
      "Epoch[1/1, Loss: 1.7724, accuracy: 0.7318]\n",
      "Epoch[1/1, Loss: 1.7718, accuracy: 0.7324]\n",
      "Epoch[1/1, Loss: 1.7711, accuracy: 0.7328]\n",
      "Epoch[1/1, Loss: 1.7704, accuracy: 0.7332]\n",
      "Epoch[1/1, Loss: 1.7697, accuracy: 0.7334]\n",
      "Epoch[1/1, Loss: 1.7691, accuracy: 0.7332]\n",
      "Epoch[1/1, Loss: 1.7685, accuracy: 0.7328]\n",
      "Epoch[1/1, Loss: 1.7678, accuracy: 0.7325]\n",
      "Epoch[1/1, Loss: 1.7672, accuracy: 0.7330]\n",
      "Epoch[1/1, Loss: 1.7665, accuracy: 0.7332]\n",
      "Epoch[1/1, Loss: 1.7658, accuracy: 0.7325]\n",
      "Epoch[1/1, Loss: 1.7652, accuracy: 0.7336]\n",
      "Epoch[1/1, Loss: 1.7646, accuracy: 0.7344]\n",
      "Epoch[1/1, Loss: 1.7639, accuracy: 0.7347]\n",
      "Epoch[1/1, Loss: 1.7632, accuracy: 0.7348]\n",
      "Epoch[1/1, Loss: 1.7625, accuracy: 0.7346]\n",
      "Epoch[1/1, Loss: 1.7619, accuracy: 0.7348]\n",
      "Epoch[1/1, Loss: 1.7612, accuracy: 0.7347]\n",
      "Epoch[1/1, Loss: 1.7605, accuracy: 0.7350]\n",
      "Epoch[1/1, Loss: 1.7598, accuracy: 0.7347]\n",
      "Epoch[1/1, Loss: 1.7592, accuracy: 0.7356]\n",
      "Epoch[1/1, Loss: 1.7586, accuracy: 0.7358]\n",
      "Epoch[1/1, Loss: 1.7580, accuracy: 0.7359]\n",
      "Epoch[1/1, Loss: 1.7573, accuracy: 0.7360]\n",
      "Epoch[1/1, Loss: 1.7567, accuracy: 0.7358]\n",
      "Epoch[1/1, Loss: 1.7560, accuracy: 0.7361]\n",
      "Epoch[1/1, Loss: 1.7553, accuracy: 0.7364]\n",
      "Epoch[1/1, Loss: 1.7546, accuracy: 0.7362]\n",
      "Epoch[1/1, Loss: 1.7540, accuracy: 0.7366]\n",
      "Epoch[1/1, Loss: 1.7534, accuracy: 0.7370]\n",
      "Epoch[1/1, Loss: 1.7527, accuracy: 0.7369]\n",
      "Epoch[1/1, Loss: 1.7521, accuracy: 0.7370]\n",
      "Epoch[1/1, Loss: 1.7515, accuracy: 0.7375]\n",
      "Epoch[1/1, Loss: 1.7509, accuracy: 0.7372]\n",
      "Epoch[1/1, Loss: 1.7503, accuracy: 0.7372]\n",
      "Epoch[1/1, Loss: 1.7496, accuracy: 0.7378]\n",
      "Epoch[1/1, Loss: 1.7489, accuracy: 0.7378]\n",
      "Epoch[1/1, Loss: 1.7483, accuracy: 0.7382]\n",
      "Epoch[1/1, Loss: 1.7477, accuracy: 0.7385]\n",
      "Epoch[1/1, Loss: 1.7471, accuracy: 0.7388]\n",
      "Epoch[1/1, Loss: 1.7464, accuracy: 0.7386]\n",
      "Epoch[1/1, Loss: 1.7458, accuracy: 0.7388]\n",
      "Epoch[1/1, Loss: 1.7451, accuracy: 0.7388]\n",
      "Epoch[1/1, Loss: 1.7445, accuracy: 0.7391]\n",
      "Epoch[1/1, Loss: 1.7438, accuracy: 0.7397]\n",
      "Epoch[1/1, Loss: 1.7432, accuracy: 0.7399]\n",
      "Epoch[1/1, Loss: 1.7425, accuracy: 0.7392]\n",
      "Epoch[1/1, Loss: 1.7419, accuracy: 0.7392]\n",
      "Epoch[1/1, Loss: 1.7412, accuracy: 0.7397]\n",
      "Epoch[1/1, Loss: 1.7406, accuracy: 0.7397]\n",
      "Epoch[1/1, Loss: 1.7399, accuracy: 0.7392]\n",
      "Epoch[1/1, Loss: 1.7393, accuracy: 0.7398]\n",
      "Epoch[1/1, Loss: 1.7386, accuracy: 0.7400]\n",
      "Epoch[1/1, Loss: 1.7380, accuracy: 0.7404]\n",
      "Epoch[1/1, Loss: 1.7374, accuracy: 0.7412]\n",
      "Epoch[1/1, Loss: 1.7368, accuracy: 0.7409]\n",
      "Epoch[1/1, Loss: 1.7361, accuracy: 0.7408]\n",
      "Epoch[1/1, Loss: 1.7355, accuracy: 0.7412]\n",
      "Epoch[1/1, Loss: 1.7348, accuracy: 0.7412]\n",
      "Epoch[1/1, Loss: 1.7342, accuracy: 0.7416]\n",
      "Epoch[1/1, Loss: 1.7335, accuracy: 0.7414]\n",
      "Epoch[1/1, Loss: 1.7329, accuracy: 0.7413]\n",
      "Epoch[1/1, Loss: 1.7323, accuracy: 0.7415]\n",
      "Epoch[1/1, Loss: 1.7317, accuracy: 0.7412]\n",
      "Epoch[1/1, Loss: 1.7310, accuracy: 0.7419]\n",
      "Epoch[1/1, Loss: 1.7304, accuracy: 0.7420]\n",
      "Epoch[1/1, Loss: 1.7298, accuracy: 0.7422]\n",
      "Epoch[1/1, Loss: 1.7291, accuracy: 0.7423]\n",
      "Epoch[1/1, Loss: 1.7285, accuracy: 0.7424]\n",
      "Epoch[1/1, Loss: 1.7279, accuracy: 0.7424]\n",
      "Epoch[1/1, Loss: 1.7273, accuracy: 0.7426]\n",
      "Epoch[1/1, Loss: 1.7266, accuracy: 0.7431]\n",
      "Epoch[1/1, Loss: 1.7260, accuracy: 0.7430]\n",
      "Epoch[1/1, Loss: 1.7254, accuracy: 0.7426]\n",
      "Epoch[1/1, Loss: 1.7248, accuracy: 0.7426]\n",
      "Epoch[1/1, Loss: 1.7242, accuracy: 0.7431]\n",
      "Epoch[1/1, Loss: 1.7235, accuracy: 0.7432]\n",
      "Epoch[1/1, Loss: 1.7230, accuracy: 0.7432]\n",
      "Epoch[1/1, Loss: 1.7223, accuracy: 0.7431]\n",
      "Epoch[1/1, Loss: 1.7217, accuracy: 0.7433]\n",
      "Epoch[1/1, Loss: 1.7211, accuracy: 0.7438]\n",
      "Epoch[1/1, Loss: 1.7205, accuracy: 0.7440]\n",
      "Epoch[1/1, Loss: 1.7198, accuracy: 0.7440]\n",
      "Epoch[1/1, Loss: 1.7192, accuracy: 0.7439]\n",
      "Epoch[1/1, Loss: 1.7186, accuracy: 0.7440]\n",
      "Epoch[1/1, Loss: 1.7179, accuracy: 0.7443]\n",
      "Epoch[1/1, Loss: 1.7173, accuracy: 0.7444]\n",
      "Epoch[1/1, Loss: 1.7166, accuracy: 0.7444]\n",
      "Epoch[1/1, Loss: 1.7160, accuracy: 0.7448]\n",
      "Epoch[1/1, Loss: 1.7154, accuracy: 0.7444]\n",
      "Epoch[1/1, Loss: 1.7148, accuracy: 0.7451]\n",
      "Epoch[1/1, Loss: 1.7142, accuracy: 0.7448]\n",
      "Epoch[1/1, Loss: 1.7136, accuracy: 0.7458]\n",
      "Epoch[1/1, Loss: 1.7130, accuracy: 0.7457]\n",
      "Epoch[1/1, Loss: 1.7123, accuracy: 0.7457]\n",
      "Epoch[1/1, Loss: 1.7118, accuracy: 0.7462]\n",
      "Epoch[1/1, Loss: 1.7111, accuracy: 0.7462]\n",
      "Epoch[1/1, Loss: 1.7105, accuracy: 0.7459]\n",
      "Epoch[1/1, Loss: 1.7099, accuracy: 0.7460]\n",
      "Epoch[1/1, Loss: 1.7093, accuracy: 0.7463]\n",
      "Epoch[1/1, Loss: 1.7087, accuracy: 0.7460]\n",
      "Epoch[1/1, Loss: 1.7080, accuracy: 0.7462]\n",
      "Epoch[1/1, Loss: 1.7074, accuracy: 0.7459]\n",
      "Epoch[1/1, Loss: 1.7068, accuracy: 0.7463]\n",
      "Epoch[1/1, Loss: 1.7062, accuracy: 0.7468]\n",
      "Epoch[1/1, Loss: 1.7056, accuracy: 0.7467]\n",
      "Epoch[1/1, Loss: 1.7050, accuracy: 0.7467]\n",
      "Epoch[1/1, Loss: 1.7044, accuracy: 0.7470]\n",
      "Epoch[1/1, Loss: 1.7038, accuracy: 0.7471]\n",
      "Epoch[1/1, Loss: 1.7032, accuracy: 0.7470]\n",
      "Epoch[1/1, Loss: 1.7026, accuracy: 0.7478]\n"
     ]
    }
   ],
   "source": [
    "fit(1,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=MNIST(root='data/',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e68ad65fa42939f3d1d64c725c73b895c742464099851271a5b593478fe3a0a1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('Tensorflow_CPU': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
